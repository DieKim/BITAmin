{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 딥러닝 작동원리\n",
    "### 1-1. ![그림1](./그림1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 조건: \n",
    "* x1 = 0.1, x2 = 0.2  \n",
    "* w1 = 0.3, w2 = 0.25, w3 = 0.4, w4 = 0.35, w5 = -0.45, w6 = 0.4, w7 = -0.7, w8 = 0.6\n",
    "* 활성화함수로는 ReLU 함수를 사용한다.\n",
    "\n",
    "#### 조건 하에서, 두 출력값을 각각 계산하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 답: 0.008, -0.010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008 0.010000000000000009\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    import numpy as np\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "x1 = 0.1; x2 = 0.2\n",
    "w1 = 0.3; w2 = 0.25; w3 = 0.4; w4 = 0.35; w5 = -0.45; w6 = 0.4; w7 = -0.7; w8 = 0.6\n",
    "\n",
    "z1 = x1*w1 + x2*w2\n",
    "z2 = x1*w3 + x2*w4\n",
    "\n",
    "h1 = relu(z1)\n",
    "h2 = relu(z2)\n",
    "\n",
    "o1 = h1*w5 + h2*w6\n",
    "o2 = h1*w7 + h2*w8\n",
    "\n",
    "print(o1, o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. <보기>의 단어들을 빈 괄호에 올바르게 채워넣으시오.\n",
    "#### <보기>   \n",
    "__입력층, 입력값, 출력층, 출력값, 은닉층, 가중치, 편향, 활성화함수__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인공신경망에서 순전파는 (입력층)에서 (출력층)의 순서로 중간변수들을 계산한다. 신경망의 (입력값)에 첫번째 (은닉층)의 (가중치)를 곱한 값들을 합하고, 그것에 (활성화 함수)를 적용한다. 적용된 값은 다음 층 뉴런들의 (입력값)이 된다. 이와 같은 계산을 (출력층)까지 반복하여 최종 (출력값)을 구한다.\n",
    "  \n",
    "  \n",
    "- 임의의 (가중치)와 (편향)들을 적용했을 때의 최종 (출력값)과 실제값 간의 차이를 '비용'이라고 한다. 비용의 구체적인 계산법은 선정한 손실함수에 따라 나뉜다. \n",
    "  \n",
    "  \n",
    "- 역전파는 (출력층)에서 (입력층)의 순서로 (가중치)에 대한 (손실=출력값-실제값)의 기울기를 계산한다. 각 층의 (미분값)이 최종 (손실함수)에 영향을 미치기 위해 (가중치)를 얼마나 변경해야 할지 측정해준다. (가중치)를 변경해야 하는 방향과 정도는 경사하강법을 통해 업데이트된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 다음 문장들이 틀린 이유를 구체적으로 서술하고, 올바르게 고치시오.\n",
    "### 2-1. 지나치게 적은 Epoch는 과적합의 원인이 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 틀린 이유: 지나치게 적은 Epoch는 학습을 충분히 못했을 수 있으므로 과적합이 아니라 오히려 학습이 더 필요하다.  \n",
    "* 고친 문장: 지나치게 __많은__ Epoch는 과적합의 원이이 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Batch의 사이즈는 한 Epoch의 Iteration 수와 같다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 틀린 이유: 대략적으로 'Batch 사이즈 * Iteration = 전체 데이터 수'이고, Batch 사이즈는 한 Epoch의 Iteration 수가 아니라 각 Iteration에서 사용하는 데이터 셋의 크기를 의미한다.\n",
    "* 고친 문장: Batch 사이즈는 한 Epoch의 각 Iteration에서 사용하는 데이터 셋의 크기와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 전체 데이터셋에 1000개의 훈련샘플이 있다. Epoch가 10, Batch의 사이즈가 20이라면, 전체 학습에 걸쳐서 가중치는 총 50번 업데이트 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 틀린 이유: 50 * 10 = 500 (한 epoch 당 iteration * 전체 epoch 수)\n",
    "* 고친 문장: 전체 학습에 걸쳐서 가중치는 총 __500번__ 업데이트 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  배치경사하강법을 적용하자 학습시간이 너무 많이 소요되고 있다.\n",
    "###  다음 중, 학습시간을 단축하면서도 적절한 예측값을 낼 수 있는 테크닉을 *모두 고르시오.*\n",
    "__i. Adam 옵티마이저를 사용해본다.  \n",
    "ii. 학습률을 재조정해본다.  \n",
    "iii. Mini Batch Gradient Descent를 사용해본다.  \n",
    "iv. 모든 가중치의 값을 0으로 초기화해본다.  \n",
    "v. 전체 데이터셋의 개수를 대폭 하향조정한다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 답: i, ii, iii\n",
    "* i: Adam 옵티마이저는 weight를 방향과 크기를 고려해서 적절히 업데이트 하기 때문에 많이 사용하는 최적화 방법이다.\n",
    "* ii: 학습률을 적절히 조절하면 예측값에 더 빠르게 도달할 수 있다. \n",
    "* iii: 기본 GD는 전체 데이터를 학습하므로 시간이 너무 많이 소모되므로 적절한 배치 크기에 따라 MSGD 방법을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 그래디언트 소실 현상에 대해 다음의 요소를 서술하시오.\n",
    "\n",
    "- 어떠한 현상인가?\n",
    "- 발생하는 이유는 무엇인가?\n",
    "- 문제가 되는 이유는 무엇인가?\n",
    "- 방지할 수 있는 방법을 하나 이상 서술하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1) 학습이 진행되면서(층이 깊어지면서) 가중치가 점점 0으로 수렴하는 현상이다.\n",
    "* 2) 보통 sigmoid를 활성함수로 쓰면 1보다 작은 값을 계속 곱하면서 값이 0으로 가까워지며 발생한다.\n",
    "* 3) 학습해야할 파라미터인 weight가 0이 되면 더 이상 학습이 진행될 수 없다. (loss가 좋아질 수 없다.)\n",
    "* 4) 적절한 activation function(sigmoid -> relu), 적절한 weight initialization, batch nomalization 적용 등 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
